{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SS_lab3",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNBoakXHkV+Z/z3bzfSiHdA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Lenaami/SpeechSynthesisLabs/blob/main/SS_lab3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nE_O2ye2NxIG",
        "outputId": "92b67b32-3854-4ef0-ce67-1acefec02f7a"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LjbICZ1UNxFf"
      },
      "source": [
        "path = '/content/gdrive/My Drive/Colab Notebooks/Синтез речи/'"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z3t3WxJKWWoF"
      },
      "source": [
        "book1 = 'r_hod.Result.xml'\n",
        "book2 = 'tropa.Result.xml'\n",
        "book3 = 'whtguard.Result.xml'"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lKVWcZSzjQr1"
      },
      "source": [
        "# Парсинг xml"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CoBO8OkFNxDE"
      },
      "source": [
        "import xml.etree.ElementTree as ET \n",
        "import numpy as np\n",
        "import re"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GDZaYA1z6ws_"
      },
      "source": [
        "def isNone(a):\n",
        "    return int(a) if a is not None else -1"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GbiBZtcKesLF"
      },
      "source": [
        "# Создание словарей\r\n",
        "\r\n",
        "letters = ['']\r\n",
        "phonemes = ['']\r\n",
        "allophones = ['']\r\n",
        "\r\n",
        "books = [book1, book2, book3]\r\n",
        "\r\n",
        "for book in books:\r\n",
        "    tree = ET.parse(path + book)\r\n",
        "    root = tree.getroot()\r\n",
        "\r\n",
        "    for snt in root.findall('sentence'):\r\n",
        "        for feat in snt:\r\n",
        "            if feat.tag == 'word':         \r\n",
        "                for lt in feat:\r\n",
        "                    if lt.tag == 'letter': \r\n",
        "                        letters.append(lt.get('char'))\r\n",
        "                    if lt.tag == 'phoneme': \r\n",
        "                        phonemes.append(lt.get('ph'))\r\n",
        "                    if lt.tag == 'allophone': \r\n",
        "                        allophones.append(lt.get('ph'))    \r\n",
        "\r\n",
        "letters = set(letters)\r\n",
        "phonemes = set(phonemes)\r\n",
        "allophones = set(allophones)\r\n",
        "\r\n",
        "letters_dict = {lt:i for i,lt in enumerate(letters)}\r\n",
        "phonemes_dict = {ph:i for i,ph in enumerate(phonemes)}\r\n",
        "allophones_dict = {ph:i for i,ph in enumerate(allophones)}\r\n",
        "\r\n",
        "num_to_letters = {i:lt for i,lt in enumerate(letters)}\r\n",
        "num_to_phonemes = {i:ph for i,ph in enumerate(phonemes)}\r\n",
        "num_to_allophones = {i:ph for i,ph in enumerate(allophones)}"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wgB56mEgf1Hw"
      },
      "source": [
        "def get_data(file):  \n",
        "    X_phoneme = []\n",
        "    X_allophone = []\n",
        "    y_phoneme = []\n",
        "    y_allophone = []\n",
        "\n",
        "    tree = ET.parse(file)\n",
        "    root = tree.getroot()\n",
        "\n",
        "    tags = ['word', 'pause']\n",
        "\n",
        "    for snt in root.findall('sentence'):\n",
        "        features = []\n",
        "        feat_wrd = []\n",
        "\n",
        "        features_lt = []\n",
        "        features_lt_stc = []\n",
        "        letters = []\n",
        "        feat_lt = []\n",
        "        let_wrd = []\n",
        "        ph_wrd = []\n",
        "        allph_wrd = []\n",
        "\n",
        "        ys_phoneme = []\n",
        "        ys_allophone = []\n",
        "\n",
        "        pause_pred = 0\n",
        "        count = 0\n",
        "        for feat in snt:\n",
        "\n",
        "            if feat.tag not in tags:\n",
        "                continue\n",
        "\n",
        "            if feat.tag == 'word':\n",
        "                if pause_pred:\n",
        "                    feat_wrd.append(1) # пауза до слова\n",
        "                    feat_wrd.append(0) # пауза после слова (предв.)\n",
        "                    pause_pred = 0\n",
        "                else:\n",
        "                    feat_wrd.append(0)\n",
        "                    feat_wrd.append(0)\n",
        "\n",
        "                dct = feat.find('dictitem')\n",
        "                feat_wrd.append(isNone(dct.get('stress_dict'))) # ударение\n",
        "                feat_wrd.append(isNone(dct.get('genesys'))) # одушевленность\n",
        "                feat_wrd.append(isNone(dct.get('form'))) # форма\n",
        "                feat_wrd.append(isNone(dct.get('subpart_of_speech'))) # часть речи        \n",
        "                feat_wrd.append(isNone(dct.get('semantics1')))\n",
        "                feat_wrd.append(isNone(dct.get('semantics2')))\n",
        "                feat_wrd.append(-1) # ударение (предв.)\n",
        "                \n",
        "                let_wrd = [''] # нет символа до слова\n",
        "                ph_wrd = ['']\n",
        "                allph_wrd = []\n",
        "                features_lt = []\n",
        "                gl = 0\n",
        "                sogl = 0\n",
        "                for lt in feat:                    \n",
        "                    \n",
        "                    if lt.tag == 'letter':\n",
        "                        feat_lt = []\n",
        "                        let_wrd.append(lt.get('char'))  \n",
        "                        feat_lt.append(sogl + gl) # позиция символа в слове\n",
        "                        feat_lt.append(sogl) # число согласных до\n",
        "                        feat_lt.append(gl) # число гласных до\n",
        "                        if lt.get('flag') == '16' or lt.get('flag') is None:  # флаги букв  (не равно 16, none - гласная)\n",
        "                            sogl += 1\n",
        "                            feat_lt.append(1) # согласная\n",
        "                        else:\n",
        "                            gl += 1\n",
        "                            feat_lt.append(0) # гласная  \n",
        "                        features_lt.append(feat_lt)\n",
        "\n",
        "                        ph_wrd.append('')\n",
        "                        allph_wrd.append('')\n",
        "\n",
        "                    if lt.tag == 'phoneme':\n",
        "                        ph_wrd[-1] = lt.get('ph')\n",
        "\n",
        "                    if lt.tag == 'allophone':\n",
        "                        allph_wrd[-1] = lt.get('ph')\n",
        "\n",
        "                    if lt.tag == 'stress':\n",
        "                        feat_wrd[-1] = len(let_wrd) # позиция ударной буквы                    \n",
        "\n",
        "                let_wrd.append('') # нет символа после слова\n",
        "                ph_wrd.append('')\n",
        "\n",
        "                feat_wrd.append(sogl) # число согласных\n",
        "                feat_wrd.append(gl) # число гласных\n",
        "\n",
        "                feat_wrd.append(count) # позиция слова в предложении\n",
        "                count += 1 # подсчет слов              \n",
        "               \n",
        "                features.append(feat_wrd)\n",
        "                feat_wrd = []\n",
        "\n",
        "                letters.append(let_wrd)\n",
        "                features_lt_stc.append(features_lt)\n",
        "                ys_phoneme.append(ph_wrd)\n",
        "                ys_allophone.append(allph_wrd)\n",
        "\n",
        "\n",
        "            if feat.tag == 'pause':\n",
        "                features[-1][1] = 1 # пауза после слова\n",
        "                pause_pred = 1             \n",
        "\n",
        "        # объединение \n",
        "        for i in range(count):\n",
        "            wrd_ph = []\n",
        "            wrd_allph = []\n",
        "            for l in range(len(letters[i]) - 2):\n",
        "                feat = [letters[i][l+1], letters[i][l], letters[i][l+2]] # текущая буква, перед и после\n",
        "                feat = [letters_dict.get(s) for s in feat] # перевод из буквы в цифру\n",
        "                feat.extend(features_lt_stc[i][l])\n",
        "                feat.extend(features[i])\n",
        "                feat.append(count)\n",
        "                wrd_ph.append(feat)\n",
        "\n",
        "                feat = [ys_phoneme[i][l+1], ys_phoneme[i][l], ys_phoneme[i][l+2]] # текущая фонема, перед и после\n",
        "                feat = [phonemes_dict.get(s) for s in feat]\n",
        "                feat.extend(features_lt_stc[i][l])\n",
        "                feat.extend(features[i])\n",
        "                feat.append(count)\n",
        "                wrd_allph.append(feat)\n",
        "           \n",
        "            ys_phoneme[i] = [phonemes_dict.get(s) for s in ys_phoneme[i][1:-1]]\n",
        "            ys_allophone[i] = [allophones_dict.get(s) for s in ys_allophone[i]]\n",
        "            y_allophone.extend(ys_allophone[i])\n",
        "            X_phoneme.append(wrd_ph)\n",
        "            X_allophone.extend(wrd_allph)\n",
        "\n",
        "        y_phoneme.extend(ys_phoneme)\n",
        "       \n",
        "    return X_phoneme, X_allophone, y_phoneme, y_allophone"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PvRXn9cAmDPr"
      },
      "source": [
        "X_train_ph_1, X_train_allph_1, y_train_ph_1, y_train_allph_1 = get_data(path + book2)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NintYyjTvlI5"
      },
      "source": [
        "X_train_ph_2, X_train_allph_2, y_train_ph_2, y_train_allph_2 = get_data(path + book3)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZ-xDOu82Hu0"
      },
      "source": [
        "# Объединение корпусов (2 и 3)\r\n",
        "\r\n",
        "X_train_ph = []\r\n",
        "X_train_ph.extend(X_train_ph_1)\r\n",
        "X_train_ph.extend(X_train_ph_2)\r\n",
        "\r\n",
        "y_train_ph = []\r\n",
        "y_train_ph.extend(y_train_ph_1)\r\n",
        "y_train_ph.extend(y_train_ph_2)\r\n",
        "\r\n",
        "X_train_allph = []\r\n",
        "X_train_allph.extend(X_train_allph_1)\r\n",
        "X_train_allph.extend(X_train_allph_2)\r\n",
        "\r\n",
        "y_train_allph = []\r\n",
        "y_train_allph.extend(y_train_allph_1)\r\n",
        "y_train_allph.extend(y_train_allph_2)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vq_lFG8psmCP"
      },
      "source": [
        "# Тестовый корпус (1)\n",
        "\n",
        "X_test_ph, X_test_allph, y_test_ph, y_test_allph = get_data(path + book1)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8-nlLAauo5l"
      },
      "source": [
        "# Определение фонем"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6mZoib7TA72"
      },
      "source": [
        "## LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WuqIVjpqunf3"
      },
      "source": [
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "import torch.optim as optim\r\n",
        "\r\n",
        "import torchvision\r\n",
        "from sklearn.metrics import f1_score\r\n",
        "from tqdm import tqdm\r\n",
        "\r\n",
        "torch.manual_seed(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jkg1NWKfzSdt"
      },
      "source": [
        "class LSTMModel(nn.Module):\r\n",
        "\r\n",
        "    def __init__(self, embedding_dim, hidden_dim, tagset_size):\r\n",
        "        super(LSTMModel, self).__init__()\r\n",
        "        self.hidden_dim = hidden_dim\r\n",
        "        self.embedding_dim = embedding_dim\r\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\r\n",
        "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\r\n",
        "\r\n",
        "    def forward(self, word):\r\n",
        "        feats = torch.tensor(data=word, dtype=torch.float)\r\n",
        "        lstm_out, _ = self.lstm(feats.view(-1, 1, self.embedding_dim))\r\n",
        "        tag_space = self.hidden2tag(lstm_out.view(len(word), -1))\r\n",
        "        tag_scores = F.log_softmax(tag_space, dim=1)\r\n",
        "        return tag_scores"
      ],
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oenhh9cTMs0l"
      },
      "source": [
        "def eval_model(model, eval_dataset):\r\n",
        "    model.eval()\r\n",
        "    forecast, true_labels = [], []\r\n",
        "    with torch.no_grad():\r\n",
        "        for features, labels in tqdm(eval_dataset):\r\n",
        "            features = torch.tensor(features).cuda()\r\n",
        "            true_labels.append(labels)\r\n",
        "            outputs = model(features)\r\n",
        "            \r\n",
        "            outputs = outputs.detach().cpu().numpy().argmax(axis=1)\r\n",
        "            forecast.append(outputs)\r\n",
        "    forecast = [x for sublist in forecast for x in sublist]\r\n",
        "    true_labels = [x for sublist in true_labels for x in sublist]\r\n",
        "    return f1_score(forecast, true_labels, average='weighted')"
      ],
      "execution_count": 202,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YEl3dcGUM51U"
      },
      "source": [
        "# Проверка работы нейронной сети\r\n",
        "\r\n",
        "lr = 1e-3\r\n",
        "EMBEDDING_DIM = 20\r\n",
        "HIDDEN_DIM = 20\r\n",
        "\r\n",
        "model = LSTMModel(EMBEDDING_DIM, HIDDEN_DIM, len(phonemes_dict))\r\n",
        "model = model.cuda()\r\n",
        "criterion = nn.NLLLoss()\r\n",
        "optimizer = optim.SGD(model.parameters(), lr=lr)\r\n",
        "\r\n",
        "with torch.no_grad():\r\n",
        "    inputs = torch.tensor(X_train_ph[0]).cuda()\r\n",
        "    tag_scores = model(inputs)\r\n",
        "    print(tag_scores)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WdKIZtpgnOsh"
      },
      "source": [
        "# Обучение нейронной сети\r\n",
        "\r\n",
        "lr = 1e-3\r\n",
        "EMBEDDING_DIM = 20\r\n",
        "HIDDEN_DIM = 200\r\n",
        "\r\n",
        "model = LSTMModel(EMBEDDING_DIM, HIDDEN_DIM, len(phonemes_dict))\r\n",
        "model = model.cuda()\r\n",
        "criterion = nn.NLLLoss()\r\n",
        "optimizer = optim.SGD(model.parameters(), lr=lr)\r\n",
        "\r\n",
        "n_epoch = 20\r\n",
        "for epoch in range(n_epoch):\r\n",
        "    model.train()\r\n",
        "    for features, labels in tqdm(zip(X_train_ph, y_train_ph)):\r\n",
        "        optimizer.zero_grad()\r\n",
        "        features, labels = torch.tensor(features).cuda(), torch.tensor(labels).cuda()\r\n",
        "        outputs = model(features)\r\n",
        "        loss = criterion(outputs, torch.tensor(labels))\r\n",
        "        loss.backward()\r\n",
        "        optimizer.step()\r\n",
        "    f1 = eval_model(model, zip(X_test_ph, y_test_ph))\r\n",
        "    f1_train = eval_model(model, zip(X_train_ph, y_train_ph))\r\n",
        "    print(f'\\nepoch: {epoch}, f1_test: {f1}, f1_train: {f1_train}')\r\n",
        "        \r\n",
        "    lr = lr * 0.95\r\n",
        "    for param_group in optimizer.param_groups:\r\n",
        "        param_group['lr'] = lr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bXayNBWdOKQB",
        "outputId": "230d3b04-4a9e-49ee-ba70-e462d389a247"
      },
      "source": [
        "eval_model(model, zip(X_test_ph, y_test_ph))"
      ],
      "execution_count": 212,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1268it [00:00, 1794.83it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.845719740332514"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 212
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sm6zctJGtdWX",
        "outputId": "1077238b-0519-4f2e-ce05-20f5cb921460"
      },
      "source": [
        "# Пример предсказания\r\n",
        "\r\n",
        "i = 3\r\n",
        "with torch.no_grad():    \r\n",
        "    outputs = model(torch.tensor(X_test_ph[i]).cuda())            \r\n",
        "    outputs = outputs.detach().cpu().numpy().argmax(axis=1)\r\n",
        "    print(outputs)\r\n",
        "    \r\n",
        "    out = [num_to_phonemes.get(s) for s in outputs]\r\n",
        "    print(out)\r\n",
        "    print([num_to_phonemes.get(s) for s in y_test_ph[i]])"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[30 29 20 10  0 12 39 42]\n",
            "['к', \"р'\", 'е', 'с', '', 'н', 'ы', 'й']\n",
            "['к', \"р'\", 'е', 'с', '', 'н', 'ы', 'й']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  del sys.path[0]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wPTb4taDpGJx"
      },
      "source": [
        "pip install python-Levenshtein"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tUPhJx2upMLH"
      },
      "source": [
        "import Levenshtein\r\n",
        "from sklearn import metrics"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mR5KFRqvXdev"
      },
      "source": [
        "len_word = [0]\r\n",
        "for y in y_test_ph:\r\n",
        "    len_word.append(len_word[-1] + len(y))"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6O8mCKcjPAW"
      },
      "source": [
        "def show_metric_ph(y_pred, y_true, metric='micro'):\r\n",
        "\r\n",
        "    lev_score = []\r\n",
        "\r\n",
        "    for i in range(len(len_word) - 1):\r\n",
        "        pred = \"\".join([num_to_phonemes.get(s) for s in y_pred[len_word[i]:len_word[i+1]]])\r\n",
        "        true = \"\".join([num_to_phonemes.get(s) for s in y_true[len_word[i]:len_word[i+1]]])\r\n",
        "\r\n",
        "        lev_score.append(Levenshtein.distance(pred, true))    \r\n",
        "\r\n",
        "    print('\\nF1-score (%s):\\t%0.3f' % (metric, metrics.f1_score(y_true, y_pred, average=metric)))\r\n",
        "    print('Recall (%s): \\t%0.3f' % (metric, metrics.recall_score(y_true, y_pred, average=metric)))\r\n",
        "    print('Precision (%s):\\t%0.3f' % (metric,metrics.precision_score(y_true, y_pred, average=metric)))\r\n",
        "    print('Accuracy:\\t\\t%0.3f' % metrics.accuracy_score(y_true, y_pred))\r\n",
        "    print('\\nLevenshtein distance:\\t%0.3f' % np.mean(lev_score))"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CVVfbUg5pxsi",
        "outputId": "f83c28d8-bafc-403a-e18e-f613c746610d"
      },
      "source": [
        "# Оценка работы нейронной сети\r\n",
        "\r\n",
        "y_pred = []\r\n",
        "y_true = []\r\n",
        "\r\n",
        "with torch.no_grad():\r\n",
        "    for features, labels in tqdm(zip(X_test_ph, y_test_ph)):\r\n",
        "        outputs = model(torch.tensor(features).cuda())            \r\n",
        "        outputs = outputs.detach().cpu().numpy().argmax(axis=1)\r\n",
        "\r\n",
        "        y_pred.extend(outputs)\r\n",
        "        y_true.extend(labels)\r\n",
        "\r\n",
        "show_metric_ph(y_pred, y_true, 'weighted')"
      ],
      "execution_count": 211,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1268it [00:00, 1690.80it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "F1-score (weighted):\t0.837\n",
            "Recall (weighted): \t0.841\n",
            "Precision (weighted):\t0.843\n",
            "Accuracy:\t\t0.841\n",
            "\n",
            "Levenshtein distance:\t0.943\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_s-c_X8DS820"
      },
      "source": [
        "## Классификаторы"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BANG5glggz5w"
      },
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.linear_model import LogisticRegression"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-gjp3j_fBqqF"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FnRyIfGEunRi"
      },
      "source": [
        "X_train_ph_2 = [x for sublist in X_train_ph for x in sublist]\r\n",
        "y_train_ph_2 = [x for sublist in y_train_ph for x in sublist]\r\n",
        "\r\n",
        "X_test_ph_2 = [x for sublist in X_test_ph for x in sublist]\r\n",
        "y_test_ph_2 = [x for sublist in y_test_ph for x in sublist]"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xc_9a6HeTzuH",
        "outputId": "bbeb4f19-f5db-4c0a-baf5-45d895acddb6"
      },
      "source": [
        "# Decision Tree\r\n",
        "\r\n",
        "tree_0 = DecisionTreeClassifier()\r\n",
        "\r\n",
        "tree_0 = fit_clf(tree_0, X_train_ph_2, y_train_ph_2)\r\n",
        "y_pred = predict_clf(tree_0, X_test_ph_2, y_test_ph_2, 'weighted') #'weighted'"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "F1-score (weighted):\t0.977\n",
            "Recall (weighted): \t0.977\n",
            "Precision (weighted):\t0.977\n",
            "Accuracy:\t\t0.977\n",
            "\n",
            "Levenshtein distance:\t0.123\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t5ibjcPiunHK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "584701f7-72b3-4bc6-b5b0-42583f770594"
      },
      "source": [
        "# Random Forest\r\n",
        "\r\n",
        "rf_0 = RandomForestClassifier(n_estimators=15)\r\n",
        "\r\n",
        "rf_0 = fit_clf(rf_0, X_train_ph_2, y_train_ph_2)\r\n",
        "y_pred = predict_clf(rf_0, X_test_ph_2, y_test_ph_2, 'weighted') #'weighted'"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "F1-score (weighted):\t0.959\n",
            "Recall (weighted): \t0.960\n",
            "Precision (weighted):\t0.960\n",
            "Accuracy:\t\t0.960\n",
            "\n",
            "Levenshtein distance:\t0.222\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tOJMrgtwzNa3",
        "outputId": "e99bfd33-4b56-4efc-8692-84a60fdc9550"
      },
      "source": [
        "# Naive Baies\r\n",
        "\r\n",
        "nb_0 = GaussianNB(var_smoothing=1e-5) \r\n",
        "\r\n",
        "nb_0 = fit_clf(nb_0, X_train_ph_2, y_train_ph_2)\r\n",
        "y_pred = predict_clf(nb_0, X_test_ph_2, y_test_ph_2, 'weighted')"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "F1-score (weighted):\t0.469\n",
            "Recall (weighted): \t0.481\n",
            "Precision (weighted):\t0.509\n",
            "Accuracy:\t\t0.481\n",
            "\n",
            "Levenshtein distance:\t2.811\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BVOMbl5bT8zA",
        "outputId": "3fb71ace-fd16-4258-e0cc-70eb5a9deea4"
      },
      "source": [
        "# Logistic Regression\r\n",
        "\r\n",
        "lr_0 = LogisticRegression()\r\n",
        "\r\n",
        "lr_0 = fit_clf(lr_0, X_train_ph_2, y_train_ph_2)\r\n",
        "y_pred = predict_clf(lr_0, X_test_ph_2, y_test_ph_2, 'weighted')"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "F1-score (weighted):\t0.264\n",
            "Recall (weighted): \t0.307\n",
            "Precision (weighted):\t0.246\n",
            "Accuracy:\t\t0.307\n",
            "\n",
            "Levenshtein distance:\t4.014\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UGcOvwg6isBe"
      },
      "source": [
        "#Классификация фонем"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KLqcXG-7njFq"
      },
      "source": [
        "def fit_clf(clf, X_train, y_train):\n",
        "    clf.fit(X_train, y_train)\n",
        "    return clf\n",
        "\n",
        "def predict_clf(clf, X_test, y_test, metric='micro'):\n",
        "    y_pred = clf.predict(X_test)\n",
        "    show_metric(y_pred, y_test, metric)\n",
        "    return y_pred\n",
        "\n",
        "def show_metric(y_pred, y_true, metric):\n",
        "\n",
        "    lev_score = []\n",
        "    for i in range(len(len_word) - 1):\n",
        "        pred = \"\".join([num_to_allophones.get(s) for s in y_pred[len_word[i]:len_word[i+1]]])\n",
        "        true = \"\".join([num_to_allophones.get(s) for s in y_true[len_word[i]:len_word[i+1]]])\n",
        "        lev_score.append(Levenshtein.distance(pred, true))    \n",
        "\n",
        "    print('F1-score (%s):\\t%0.3f' % (metric, metrics.f1_score(y_true, y_pred, average=metric)))\n",
        "    print('Recall (%s): \\t%0.3f' % (metric, metrics.recall_score(y_true, y_pred, average=metric)))\n",
        "    print('Precision (%s):\\t%0.3f' % (metric,metrics.precision_score(y_true, y_pred, average=metric)))\n",
        "    print('Accuracy:\\t\\t%0.3f' % metrics.accuracy_score(y_true, y_pred))\n",
        "    print('\\nLevenshtein distance:\\t%0.3f' % np.mean(lev_score))"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LlHx1Vm1i6NL"
      },
      "source": [
        "## Decision Tree"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j-zop2EGHPTP",
        "outputId": "b4fe1c76-55d2-4812-f9ba-c3a6c4238e6a"
      },
      "source": [
        "tree = DecisionTreeClassifier()\n",
        "\n",
        "tree = fit_clf(tree, X_train_allph, y_train_allph)\n",
        "y_pred = predict_clf(tree, X_test_allph, y_test_allph, 'weighted')"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "F1-score (weighted):\t0.992\n",
            "Recall (weighted): \t0.992\n",
            "Precision (weighted):\t0.991\n",
            "Accuracy:\t\t0.992\n",
            "\n",
            "Levenshtein distance:\t0.041\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "beaCtn5Wi8K-"
      },
      "source": [
        "## Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xisO3HDg5LPQ",
        "outputId": "24c19c30-6856-4fb2-88a8-f9fa92c21d2b"
      },
      "source": [
        "rf = RandomForestClassifier(n_estimators=15)\n",
        "\n",
        "rf = fit_clf(rf, X_train_allph, y_train_allph)\n",
        "y_pred = predict_clf(rf, X_test_allph, y_test_allph, 'weighted')"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "F1-score (weighted):\t0.973\n",
            "Recall (weighted): \t0.974\n",
            "Precision (weighted):\t0.973\n",
            "Accuracy:\t\t0.974\n",
            "\n",
            "Levenshtein distance:\t0.179\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TaLLlmsei-it"
      },
      "source": [
        "## Naive Baies \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wQFaZu8Bm8Oq",
        "outputId": "3bbe24d8-52e6-41ca-a6c2-c6210e0e007e"
      },
      "source": [
        "nb = GaussianNB(var_smoothing=1e-5) \n",
        "\n",
        "nb = fit_clf(nb, X_train_allph, y_train_allph)\n",
        "y_pred = predict_clf(nb, X_test_allph, y_test_allph, 'weighted')"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "F1-score (weighted):\t0.862\n",
            "Recall (weighted): \t0.858\n",
            "Precision (weighted):\t0.871\n",
            "Accuracy:\t\t0.858\n",
            "\n",
            "Levenshtein distance:\t0.756\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwT6TDJ2jDbq"
      },
      "source": [
        "## Logisric Regression\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KpPP6aqEPZSK",
        "outputId": "ba436c66-90af-4b41-eb67-def513474fb3"
      },
      "source": [
        "lr = LogisticRegression()\n",
        "\n",
        "lr = fit_clf(lr, X_train_allph, y_train_allph)\n",
        "y_pred = predict_clf(lr, X_test_allph, y_test_allph, 'weighted')"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "F1-score (weighted):\t0.303\n",
            "Recall (weighted): \t0.338\n",
            "Precision (weighted):\t0.300\n",
            "Accuracy:\t\t0.338\n",
            "\n",
            "Levenshtein distance:\t5.032\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}