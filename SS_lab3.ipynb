{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SS_lab3",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNOvNlxbGMb0k4iVa5tJsOD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Lenaami/SpeechSynthesisLabs/blob/main/SS_lab3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nE_O2ye2NxIG",
        "outputId": "92b67b32-3854-4ef0-ce67-1acefec02f7a"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LjbICZ1UNxFf"
      },
      "source": [
        "path = '/content/gdrive/My Drive/Colab Notebooks/Синтез речи/'"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z3t3WxJKWWoF"
      },
      "source": [
        "book1 = 'r_hod.Result.xml'\n",
        "book2 = 'tropa.Result.xml'\n",
        "book3 = 'whtguard.Result.xml'"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lKVWcZSzjQr1"
      },
      "source": [
        "# Парсинг xml"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CoBO8OkFNxDE"
      },
      "source": [
        "import xml.etree.ElementTree as ET \n",
        "import numpy as np\n",
        "import re"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GDZaYA1z6ws_"
      },
      "source": [
        "def isNone(a):\n",
        "    return int(a) if a is not None else -1"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GbiBZtcKesLF"
      },
      "source": [
        "# Создание словарей\r\n",
        "\r\n",
        "letters = ['']\r\n",
        "phonemes = ['']\r\n",
        "allophones = ['']\r\n",
        "\r\n",
        "books = [book1, book2, book3]\r\n",
        "#books = [book1]\r\n",
        "\r\n",
        "for book in books:\r\n",
        "    tree = ET.parse(path + book)\r\n",
        "    root = tree.getroot()\r\n",
        "\r\n",
        "    for snt in root.findall('sentence'):\r\n",
        "        for feat in snt:\r\n",
        "            if feat.tag == 'word':         \r\n",
        "                for lt in feat:\r\n",
        "                    if lt.tag == 'letter': \r\n",
        "                        letters.append(lt.get('char'))\r\n",
        "                    if lt.tag == 'phoneme': \r\n",
        "                        phonemes.append(lt.get('ph'))\r\n",
        "                    if lt.tag == 'allophone': \r\n",
        "                        allophones.append(lt.get('ph'))    \r\n",
        "\r\n",
        "letters = set(letters)\r\n",
        "phonemes = set(phonemes)\r\n",
        "allophones = set(allophones)\r\n",
        "\r\n",
        "letters_dict = {lt:i for i,lt in enumerate(letters)}\r\n",
        "phonemes_dict = {ph:i for i,ph in enumerate(phonemes)}\r\n",
        "allophones_dict = {ph:i for i,ph in enumerate(allophones)}\r\n",
        "\r\n",
        "num_to_letters = {i:lt for i,lt in enumerate(letters)}\r\n",
        "num_to_phonemes = {i:ph for i,ph in enumerate(phonemes)}\r\n",
        "num_to_allophones = {i:ph for i,ph in enumerate(allophones)}"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wgB56mEgf1Hw"
      },
      "source": [
        "def get_data(file):  \n",
        "    X_phoneme = []\n",
        "    X_allophone = []\n",
        "    y_phoneme = []\n",
        "    y_allophone = []\n",
        "\n",
        "    tree = ET.parse(file)\n",
        "    root = tree.getroot()\n",
        "\n",
        "    tags = ['word', 'pause']\n",
        "\n",
        "    for snt in root.findall('sentence'):\n",
        "        features = []\n",
        "        feat_wrd = []\n",
        "\n",
        "        features_lt = []\n",
        "        features_lt_stc = []\n",
        "        letters = []\n",
        "        feat_lt = []\n",
        "        let_wrd = []\n",
        "        ph_wrd = []\n",
        "        allph_wrd = []\n",
        "\n",
        "        ys_phoneme = []\n",
        "        ys_allophone = []\n",
        "\n",
        "        pause_pred = 0\n",
        "        count = 0\n",
        "        for feat in snt:\n",
        "\n",
        "            if feat.tag not in tags:\n",
        "                continue\n",
        "\n",
        "            if feat.tag == 'word':\n",
        "                if pause_pred:\n",
        "                    feat_wrd.append(1) # пауза до слова\n",
        "                    feat_wrd.append(0) # пауза после слова (предв.)\n",
        "                    pause_pred = 0\n",
        "                else:\n",
        "                    feat_wrd.append(0)\n",
        "                    feat_wrd.append(0)\n",
        "\n",
        "                dct = feat.find('dictitem')\n",
        "                feat_wrd.append(isNone(dct.get('stress_dict'))) # ударение\n",
        "                feat_wrd.append(isNone(dct.get('genesys'))) # одушевленность\n",
        "                feat_wrd.append(isNone(dct.get('form'))) # форма\n",
        "                feat_wrd.append(isNone(dct.get('subpart_of_speech'))) # часть речи        \n",
        "                feat_wrd.append(isNone(dct.get('semantics1')))\n",
        "                feat_wrd.append(isNone(dct.get('semantics2')))\n",
        "                feat_wrd.append(-1) # ударение (предв.)\n",
        "                \n",
        "                let_wrd = [''] # нет символа до слова\n",
        "                ph_wrd = ['']\n",
        "                allph_wrd = []\n",
        "                features_lt = []\n",
        "                gl = 0\n",
        "                sogl = 0\n",
        "                for lt in feat:                    \n",
        "                    \n",
        "                    if lt.tag == 'letter':\n",
        "                        feat_lt = []\n",
        "                        let_wrd.append(lt.get('char'))  \n",
        "                        feat_lt.append(sogl + gl) # позиция символа в слове\n",
        "                        feat_lt.append(sogl) # число согласных до\n",
        "                        feat_lt.append(gl) # число гласных до\n",
        "                        if lt.get('flag') == '16' or lt.get('flag') is None:  # флаги букв  (не равно 16, none - гласная)\n",
        "                            sogl += 1\n",
        "                            feat_lt.append(1) # согласная\n",
        "                        else:\n",
        "                            gl += 1\n",
        "                            feat_lt.append(0) # гласная  \n",
        "                        features_lt.append(feat_lt)\n",
        "\n",
        "                        ph_wrd.append('')\n",
        "                        allph_wrd.append('')\n",
        "\n",
        "                    if lt.tag == 'phoneme':\n",
        "                        ph_wrd[-1] = lt.get('ph')\n",
        "\n",
        "                    if lt.tag == 'allophone':\n",
        "                        allph_wrd[-1] = lt.get('ph')\n",
        "\n",
        "                    if lt.tag == 'stress':\n",
        "                        feat_wrd[-1] = len(let_wrd) # позиция ударной буквы                    \n",
        "\n",
        "                let_wrd.append('') # нет символа после слова\n",
        "                ph_wrd.append('')\n",
        "\n",
        "                feat_wrd.append(sogl) # число согласных\n",
        "                feat_wrd.append(gl) # число гласных\n",
        "\n",
        "                feat_wrd.append(count) # позиция слова в предложении\n",
        "                count += 1 # подсчет слов              \n",
        "               \n",
        "                features.append(feat_wrd)\n",
        "                feat_wrd = []\n",
        "\n",
        "                letters.append(let_wrd)\n",
        "                features_lt_stc.append(features_lt)\n",
        "                ys_phoneme.append(ph_wrd)\n",
        "                ys_allophone.append(allph_wrd)\n",
        "\n",
        "\n",
        "            if feat.tag == 'pause':\n",
        "                features[-1][1] = 1 # пауза после слова\n",
        "                pause_pred = 1             \n",
        "\n",
        "        # объединение \n",
        "        for i in range(count):\n",
        "            wrd_ph = []\n",
        "            wrd_allph = []\n",
        "            for l in range(len(letters[i]) - 2):\n",
        "                feat = [letters[i][l+1], letters[i][l], letters[i][l+2]] # текущая буква, перед и после\n",
        "                feat = [letters_dict.get(s) for s in feat] # перевод из буквы в цифру\n",
        "                feat.extend(features_lt_stc[i][l])\n",
        "                feat.extend(features[i])\n",
        "                feat.append(count)\n",
        "                wrd_ph.append(feat)\n",
        "\n",
        "                feat = [ys_phoneme[i][l+1], ys_phoneme[i][l], ys_phoneme[i][l+2]] # текущая фонема, перед и после\n",
        "                feat = [phonemes_dict.get(s) for s in feat]\n",
        "                feat.extend(features_lt_stc[i][l])\n",
        "                feat.extend(features[i])\n",
        "                feat.append(count)\n",
        "                wrd_allph.append(feat)\n",
        "           \n",
        "            ys_phoneme[i] = [phonemes_dict.get(s) for s in ys_phoneme[i][1:-1]]\n",
        "            ys_allophone[i] = [allophones_dict.get(s) for s in ys_allophone[i]]\n",
        "            y_allophone.extend(ys_allophone[i])\n",
        "            X_phoneme.append(wrd_ph)\n",
        "            X_allophone.extend(wrd_allph)\n",
        "\n",
        "        y_phoneme.extend(ys_phoneme)\n",
        "       \n",
        "    return X_phoneme, X_allophone, y_phoneme, y_allophone"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PvRXn9cAmDPr"
      },
      "source": [
        "X_train_ph_1, X_train_allph_1, y_train_ph_1, y_train_allph_1 = get_data(path + book2)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NintYyjTvlI5"
      },
      "source": [
        "X_train_ph_2, X_train_allph_2, y_train_ph_2, y_train_allph_2 = get_data(path + book3)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZ-xDOu82Hu0"
      },
      "source": [
        "# Объединение корпусов (2 и 3)\r\n",
        "\r\n",
        "X_train_ph = []\r\n",
        "X_train_ph.extend(X_train_ph_1)\r\n",
        "X_train_ph.extend(X_train_ph_2)\r\n",
        "\r\n",
        "y_train_ph = []\r\n",
        "y_train_ph.extend(y_train_ph_1)\r\n",
        "y_train_ph.extend(y_train_ph_2)\r\n",
        "\r\n",
        "X_train_allph = []\r\n",
        "X_train_allph.extend(X_train_allph_1)\r\n",
        "X_train_allph.extend(X_train_allph_2)\r\n",
        "\r\n",
        "y_train_allph = []\r\n",
        "y_train_allph.extend(y_train_allph_1)\r\n",
        "y_train_allph.extend(y_train_allph_2)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vq_lFG8psmCP"
      },
      "source": [
        "# Тестовый корпус (1)\n",
        "\n",
        "X_test_ph, X_test_allph, y_test_ph, y_test_allph = get_data(path + book1)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8-nlLAauo5l"
      },
      "source": [
        "# Определение фонем"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6mZoib7TA72"
      },
      "source": [
        "## LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WuqIVjpqunf3"
      },
      "source": [
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "import torch.optim as optim\r\n",
        "\r\n",
        "import torchvision\r\n",
        "from sklearn.metrics import f1_score\r\n",
        "from tqdm import tqdm\r\n",
        "\r\n",
        "torch.manual_seed(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jkg1NWKfzSdt"
      },
      "source": [
        "class LSTMModel(nn.Module):\r\n",
        "\r\n",
        "    def __init__(self, embedding_dim, hidden_dim, tagset_size):\r\n",
        "        super(LSTMModel, self).__init__()\r\n",
        "        self.hidden_dim = hidden_dim\r\n",
        "        self.embedding_dim = embedding_dim\r\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\r\n",
        "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\r\n",
        "\r\n",
        "    def forward(self, word):\r\n",
        "        feats = torch.tensor(data=word, dtype=torch.float)\r\n",
        "        lstm_out, _ = self.lstm(feats.view(-1, 1, self.embedding_dim))\r\n",
        "        tag_space = self.hidden2tag(lstm_out.view(len(word), -1))\r\n",
        "        tag_scores = F.log_softmax(tag_space, dim=1)\r\n",
        "        return tag_scores"
      ],
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oenhh9cTMs0l"
      },
      "source": [
        "def eval_model(model, eval_dataset):\r\n",
        "    model.eval()\r\n",
        "    forecast, true_labels = [], []\r\n",
        "    with torch.no_grad():\r\n",
        "        for features, labels in tqdm(eval_dataset):\r\n",
        "            features = torch.tensor(features).cuda()\r\n",
        "            true_labels.append(labels)\r\n",
        "            outputs = model(features)\r\n",
        "            \r\n",
        "            outputs = outputs.detach().cpu().numpy().argmax(axis=1)\r\n",
        "            forecast.append(outputs)\r\n",
        "    forecast = [x for sublist in forecast for x in sublist]\r\n",
        "    true_labels = [x for sublist in true_labels for x in sublist]\r\n",
        "    return f1_score(forecast, true_labels, average='weighted')"
      ],
      "execution_count": 202,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YEl3dcGUM51U"
      },
      "source": [
        "lr = 1e-3\r\n",
        "EMBEDDING_DIM = 20\r\n",
        "HIDDEN_DIM = 20\r\n",
        "\r\n",
        "model = LSTMModel(EMBEDDING_DIM, HIDDEN_DIM, len(phonemes_dict))\r\n",
        "model = model.cuda()\r\n",
        "criterion = nn.NLLLoss()\r\n",
        "optimizer = optim.SGD(model.parameters(), lr=lr)\r\n",
        "\r\n",
        "with torch.no_grad():\r\n",
        "    inputs = torch.tensor(X_train_ph[0]).cuda()\r\n",
        "    tag_scores = model(inputs)\r\n",
        "    print(tag_scores)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KyywxmXhs1fT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dsGm00nu4P5M",
        "outputId": "e1dd8a3e-8d9d-4827-cbd0-ba5f3ae83c81"
      },
      "source": [
        "lr = 1e-3\r\n",
        "EMBEDDING_DIM = 20\r\n",
        "HIDDEN_DIM = 200\r\n",
        "\r\n",
        "model = LSTMModel(EMBEDDING_DIM, HIDDEN_DIM, len(phonemes_dict))\r\n",
        "model = model.cuda()\r\n",
        "criterion = nn.NLLLoss()\r\n",
        "optimizer = optim.SGD(model.parameters(), lr=lr)\r\n",
        "\r\n",
        "n_epoch = 10\r\n",
        "for epoch in range(n_epoch):\r\n",
        "    model.train()\r\n",
        "    for features, labels in tqdm(zip(X_train_ph, y_train_ph)):\r\n",
        "        optimizer.zero_grad()\r\n",
        "        features, labels = torch.tensor(features).cuda(), torch.tensor(labels).cuda()\r\n",
        "        outputs = model(features)\r\n",
        "        loss = criterion(outputs, torch.tensor(labels))\r\n",
        "        loss.backward()\r\n",
        "        optimizer.step()\r\n",
        "    f1 = eval_model(model, zip(X_test_ph, y_test_ph))\r\n",
        "    f1_train = eval_model(model, zip(X_train_ph, y_train_ph))\r\n",
        "    print(f'\\nepoch: {epoch}, f1_test: {f1}, f1_train: {f1_train}')\r\n",
        "        \r\n",
        "    lr = lr * 0.95\r\n",
        "    for param_group in optimizer.param_groups:\r\n",
        "        param_group['lr'] = lr"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0it [00:00, ?it/s]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  del sys.path[0]\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "193701it [05:45, 560.65it/s]\n",
            "1268it [00:00, 1886.36it/s]\n",
            "193701it [01:40, 1918.11it/s]\n",
            "58it [00:00, 571.40it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "epoch: 0, f1_test: 0.33034182025306397, f1_train: 0.35497657341642724\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "193701it [05:50, 553.09it/s]\n",
            "1268it [00:00, 1979.66it/s]\n",
            "193701it [01:44, 1858.91it/s]\n",
            "58it [00:00, 577.11it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "epoch: 1, f1_test: 0.46146895985815495, f1_train: 0.48025782950600737\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "193701it [05:56, 542.90it/s]\n",
            "1268it [00:00, 1934.32it/s]\n",
            "193701it [01:46, 1822.87it/s]\n",
            "53it [00:00, 524.57it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "epoch: 2, f1_test: 0.5339883789514307, f1_train: 0.5552799113902493\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "193701it [05:50, 552.69it/s]\n",
            "1268it [00:00, 1954.13it/s]\n",
            "193701it [01:40, 1925.21it/s]\n",
            "55it [00:00, 549.57it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "epoch: 3, f1_test: 0.5901240341644572, f1_train: 0.5984136473643606\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "193701it [05:45, 561.01it/s]\n",
            "1268it [00:00, 1889.15it/s]\n",
            "193701it [01:41, 1902.33it/s]\n",
            "55it [00:00, 546.73it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "epoch: 4, f1_test: 0.6203759180150296, f1_train: 0.6328341625880757\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "193701it [05:52, 550.00it/s]\n",
            "1268it [00:00, 1895.80it/s]\n",
            "193701it [01:43, 1868.55it/s]\n",
            "53it [00:00, 523.56it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "epoch: 5, f1_test: 0.6539114363477522, f1_train: 0.6612379327954812\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "193701it [05:55, 544.97it/s]\n",
            "1268it [00:00, 1863.99it/s]\n",
            "193701it [01:42, 1885.32it/s]\n",
            "54it [00:00, 532.31it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "epoch: 6, f1_test: 0.677704717760088, f1_train: 0.6835041051675144\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "193701it [05:54, 546.24it/s]\n",
            "1268it [00:00, 1848.99it/s]\n",
            "193701it [01:42, 1898.42it/s]\n",
            "53it [00:00, 526.47it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "epoch: 7, f1_test: 0.6867575272866396, f1_train: 0.6985769806314697\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "193701it [05:53, 548.63it/s]\n",
            "1268it [00:00, 1872.64it/s]\n",
            "193701it [01:43, 1867.74it/s]\n",
            "54it [00:00, 537.72it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "epoch: 8, f1_test: 0.695252810408832, f1_train: 0.7137517391737785\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "193701it [05:54, 545.81it/s]\n",
            "1268it [00:00, 1868.96it/s]\n",
            "193701it [01:43, 1870.18it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "epoch: 9, f1_test: 0.7001625680744026, f1_train: 0.7219106102451079\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Nna_Zo2Wze_",
        "outputId": "640b9b63-7506-4b6e-dfd9-bc8f5c259b50"
      },
      "source": [
        "n_epoch = 10\r\n",
        "for epoch in range(n_epoch):\r\n",
        "    model.train()\r\n",
        "    for features, labels in tqdm(zip(X_train_ph, y_train_ph)):\r\n",
        "        #print(labels)\r\n",
        "        optimizer.zero_grad()\r\n",
        "        features, labels = torch.tensor(features).cuda(), torch.tensor(labels).cuda()\r\n",
        "        outputs = model(features)\r\n",
        "        loss = criterion(outputs, torch.tensor(labels))\r\n",
        "        loss.backward()\r\n",
        "        optimizer.step()\r\n",
        "    f1 = eval_model(model, zip(X_test_ph, y_test_ph))\r\n",
        "    f1_train = eval_model(model, zip(X_train_ph, y_train_ph))\r\n",
        "    print(f'\\nepoch: {epoch + 10}, f1_test: {f1}, f1_train: {f1_train}')\r\n",
        "        \r\n",
        "    lr = lr * 0.95\r\n",
        "    for param_group in optimizer.param_groups:\r\n",
        "        param_group['lr'] = lr"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0it [00:00, ?it/s]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  del sys.path[0]\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  # Remove the CWD from sys.path while we load stuff.\n",
            "193701it [05:43, 564.00it/s]\n",
            "1268it [00:00, 1948.01it/s]\n",
            "193701it [01:40, 1921.01it/s]\n",
            "58it [00:00, 578.35it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "epoch: 10, f1_test: 0.7131621325236558, f1_train: 0.7358476606036586\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "193701it [05:41, 568.02it/s]\n",
            "1268it [00:00, 1916.34it/s]\n",
            "193701it [01:40, 1929.74it/s]\n",
            "56it [00:00, 554.62it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "epoch: 11, f1_test: 0.7163282182467872, f1_train: 0.7422957731411414\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "193701it [05:41, 566.59it/s]\n",
            "1268it [00:00, 1940.89it/s]\n",
            "193701it [01:40, 1935.87it/s]\n",
            "56it [00:00, 556.92it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "epoch: 12, f1_test: 0.7264528327723352, f1_train: 0.7504422219980843\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "193701it [05:42, 565.64it/s]\n",
            "1268it [00:00, 1887.58it/s]\n",
            "193701it [01:40, 1926.24it/s]\n",
            "57it [00:00, 563.23it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "epoch: 13, f1_test: 0.7321922648407166, f1_train: 0.7571197619870248\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "193701it [05:40, 569.64it/s]\n",
            "1268it [00:00, 1977.64it/s]\n",
            "193701it [01:39, 1939.22it/s]\n",
            "52it [00:00, 518.58it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "epoch: 14, f1_test: 0.744864179953643, f1_train: 0.7661636352291599\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "193701it [06:01, 535.94it/s]\n",
            "1268it [00:00, 1773.42it/s]\n",
            "193701it [01:42, 1895.46it/s]\n",
            "57it [00:00, 565.57it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "epoch: 15, f1_test: 0.7473623143705607, f1_train: 0.769700519420138\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "193701it [05:53, 547.65it/s]\n",
            "1268it [00:00, 1871.21it/s]\n",
            "193701it [01:43, 1867.76it/s]\n",
            "49it [00:00, 481.00it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "epoch: 16, f1_test: 0.7487524895773825, f1_train: 0.774077253008936\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "193701it [05:54, 546.58it/s]\n",
            "1268it [00:00, 1817.18it/s]\n",
            "193701it [01:43, 1879.39it/s]\n",
            "56it [00:00, 555.83it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "epoch: 17, f1_test: 0.7556772825733393, f1_train: 0.7805590794689227\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "193701it [05:50, 552.63it/s]\n",
            "1268it [00:00, 1865.93it/s]\n",
            "193701it [01:41, 1906.32it/s]\n",
            "56it [00:00, 559.77it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "epoch: 18, f1_test: 0.7549832026674964, f1_train: 0.783956822817986\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "193701it [05:44, 561.75it/s]\n",
            "1268it [00:00, 1932.53it/s]\n",
            "193701it [01:40, 1931.72it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "epoch: 19, f1_test: 0.7645419184668699, f1_train: 0.7885676858984794\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJBP9ZwenO7D"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KC5j1BYqnO13"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WrURhxdhnOw0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WdKIZtpgnOsh",
        "outputId": "1638eb9d-fdc2-4939-fe3f-5782f26b3a8b"
      },
      "source": [
        "lr = 1e-3\r\n",
        "\r\n",
        "EMBEDDING_DIM = 20\r\n",
        "HIDDEN_DIM = 200\r\n",
        "\r\n",
        "model = LSTMModel(EMBEDDING_DIM, HIDDEN_DIM, len(phonemes_dict))\r\n",
        "model = model.cuda()\r\n",
        "criterion = nn.NLLLoss()\r\n",
        "optimizer = optim.SGD(model.parameters(), lr=lr)\r\n",
        "\r\n",
        "n_epoch = 20\r\n",
        "for epoch in range(n_epoch):\r\n",
        "    model.train()\r\n",
        "    for features, labels in tqdm(zip(X_train_ph, y_train_ph)):\r\n",
        "        optimizer.zero_grad()\r\n",
        "        features, labels = torch.tensor(features).cuda(), torch.tensor(labels).cuda()\r\n",
        "        outputs = model(features)\r\n",
        "        loss = criterion(outputs, torch.tensor(labels))\r\n",
        "        loss.backward()\r\n",
        "        optimizer.step()\r\n",
        "    f1 = eval_model(model, zip(X_test_ph, y_test_ph))\r\n",
        "    f1_train = eval_model(model, zip(X_train_ph, y_train_ph))\r\n",
        "    print(f'\\nepoch: {epoch}, f1_test: {f1}, f1_train: {f1_train}')\r\n",
        "        \r\n",
        "    lr = lr * 0.95\r\n",
        "    for param_group in optimizer.param_groups:\r\n",
        "        param_group['lr'] = lr"
      ],
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0it [00:00, ?it/s]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  del sys.path[0]\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "193701it [05:59, 538.35it/s]\n",
            "1268it [00:00, 1817.82it/s]\n",
            "193701it [01:45, 1842.78it/s]\n",
            "56it [00:00, 556.82it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "epoch: 0, f1_test: 0.32485490271952433, f1_train: 0.35786701486110134\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "193701it [05:59, 538.36it/s]\n",
            "1268it [00:00, 1750.44it/s]\n",
            "193701it [01:44, 1853.67it/s]\n",
            "55it [00:00, 549.63it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "epoch: 1, f1_test: 0.4343907635018853, f1_train: 0.45490339781534583\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "193701it [05:55, 544.62it/s]\n",
            "1268it [00:00, 1883.89it/s]\n",
            "193701it [01:45, 1842.20it/s]\n",
            "46it [00:00, 452.62it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "epoch: 2, f1_test: 0.4869573839995968, f1_train: 0.5139537137663354\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "193701it [05:56, 542.93it/s]\n",
            "1268it [00:00, 1653.02it/s]\n",
            "193701it [01:45, 1843.84it/s]\n",
            "46it [00:00, 456.20it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "epoch: 3, f1_test: 0.5218694371518571, f1_train: 0.5568484546971728\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "193701it [06:03, 533.10it/s]\n",
            "1268it [00:00, 1779.56it/s]\n",
            "193701it [01:45, 1835.14it/s]\n",
            "56it [00:00, 554.47it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "epoch: 4, f1_test: 0.5479060756125804, f1_train: 0.5833534563191047\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "193701it [06:01, 536.31it/s]\n",
            "1268it [00:00, 1702.26it/s]\n",
            "193701it [01:45, 1837.10it/s]\n",
            "56it [00:00, 558.37it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "epoch: 5, f1_test: 0.5748271019551361, f1_train: 0.6159734427256462\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "193701it [06:03, 533.51it/s]\n",
            "1268it [00:00, 1642.16it/s]\n",
            "193701it [01:46, 1819.91it/s]\n",
            "46it [00:00, 451.56it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "epoch: 6, f1_test: 0.598568720429767, f1_train: 0.6431700019909852\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "193701it [06:02, 534.24it/s]\n",
            "1268it [00:00, 1896.33it/s]\n",
            "193701it [01:44, 1848.45it/s]\n",
            "57it [00:00, 562.50it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "epoch: 7, f1_test: 0.6124947318575269, f1_train: 0.6587746478753125\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "193701it [05:59, 539.15it/s]\n",
            "1268it [00:00, 1895.68it/s]\n",
            "193701it [01:45, 1842.08it/s]\n",
            "56it [00:00, 559.30it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "epoch: 8, f1_test: 0.6233287509588117, f1_train: 0.6702715317294845\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "193701it [06:04, 532.01it/s]\n",
            "1268it [00:00, 1692.05it/s]\n",
            "193701it [01:46, 1822.68it/s]\n",
            "56it [00:00, 555.54it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "epoch: 9, f1_test: 0.6325115407997662, f1_train: 0.6809638693356779\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "193701it [06:02, 534.73it/s]\n",
            "1268it [00:00, 1785.61it/s]\n",
            "193701it [01:46, 1815.67it/s]\n",
            "53it [00:00, 523.86it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "epoch: 10, f1_test: 0.6472658137752444, f1_train: 0.6966996788270998\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "193701it [06:07, 527.75it/s]\n",
            "1268it [00:00, 1588.17it/s]\n",
            "193701it [01:47, 1807.84it/s]\n",
            "46it [00:00, 455.32it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "epoch: 11, f1_test: 0.6423622858094897, f1_train: 0.6985311317902367\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "193701it [06:06, 529.14it/s]\n",
            "1268it [00:00, 1899.93it/s]\n",
            "193701it [01:47, 1799.60it/s]\n",
            "56it [00:00, 554.62it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "epoch: 12, f1_test: 0.6647540143543657, f1_train: 0.709178998961036\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "193701it [06:07, 526.68it/s]\n",
            "1268it [00:00, 1810.37it/s]\n",
            "193701it [01:46, 1823.80it/s]\n",
            "50it [00:00, 499.64it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "epoch: 13, f1_test: 0.6683474934721537, f1_train: 0.716105585871242\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "193701it [06:04, 530.69it/s]\n",
            "1268it [00:00, 1855.60it/s]\n",
            "193701it [01:47, 1798.53it/s]\n",
            "54it [00:00, 538.27it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "epoch: 14, f1_test: 0.6837301700958323, f1_train: 0.7277363453704793\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "193701it [06:04, 530.83it/s]\n",
            "1268it [00:00, 1982.95it/s]\n",
            "193701it [01:45, 1838.28it/s]\n",
            "57it [00:00, 563.24it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "epoch: 15, f1_test: 0.678174784245359, f1_train: 0.7275257367289758\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "193701it [06:03, 532.55it/s]\n",
            "1268it [00:00, 1967.35it/s]\n",
            "193701it [01:45, 1837.55it/s]\n",
            "57it [00:00, 564.18it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "epoch: 16, f1_test: 0.6894827941178449, f1_train: 0.7371243503258289\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "193701it [06:03, 533.30it/s]\n",
            "1268it [00:00, 1682.44it/s]\n",
            "193701it [01:46, 1812.19it/s]\n",
            "46it [00:00, 457.85it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "epoch: 17, f1_test: 0.6899248585085316, f1_train: 0.7418452569874339\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "193701it [06:05, 530.36it/s]\n",
            "1268it [00:00, 1757.09it/s]\n",
            "193701it [01:47, 1807.05it/s]\n",
            "55it [00:00, 548.60it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "epoch: 18, f1_test: 0.694913179631392, f1_train: 0.7456312555837468\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "193701it [06:04, 530.96it/s]\n",
            "1268it [00:00, 1926.63it/s]\n",
            "193701it [01:46, 1813.40it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "epoch: 19, f1_test: 0.702576924479888, f1_train: 0.7508214001916408\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CNDcclV0nOhh"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ikkDUmJenOSi"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w6lFSiW7tJQr",
        "outputId": "27214e91-71f9-4c9e-d555-a989b9013551"
      },
      "source": [
        "eval_model(model, zip(X_test_ph, y_test_ph))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0it [00:00, ?it/s]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  del sys.path[0]\n",
            "1268it [00:00, 1841.27it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7645419184668699"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sm6zctJGtdWX",
        "outputId": "1077238b-0519-4f2e-ce05-20f5cb921460"
      },
      "source": [
        "i = 3\r\n",
        "with torch.no_grad():    \r\n",
        "    outputs = model(torch.tensor(X_test_ph[i]).cuda())            \r\n",
        "    outputs = outputs.detach().cpu().numpy().argmax(axis=1)\r\n",
        "    print(outputs)\r\n",
        "    \r\n",
        "    out = [num_to_phonemes.get(s) for s in outputs]\r\n",
        "    print(out)\r\n",
        "    print([num_to_phonemes.get(s) for s in y_test_ph[i]])"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[30 29 20 10  0 12 39 42]\n",
            "['к', \"р'\", 'е', 'с', '', 'н', 'ы', 'й']\n",
            "['к', \"р'\", 'е', 'с', '', 'н', 'ы', 'й']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  del sys.path[0]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWUPWIfOGGbO"
      },
      "source": [
        "torch.save(model.state_dict(), path + 'model_test.pt')"
      ],
      "execution_count": 157,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wpME7XugM8xj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39bca058-555a-4f42-9a61-697a16b62623"
      },
      "source": [
        "# Загрузить модель\r\n",
        "model_name = 'model_test.pt'\r\n",
        "\r\n",
        "EMBEDDING_DIM = 20\r\n",
        "HIDDEN_DIM = 200\r\n",
        "model = LSTMModel(EMBEDDING_DIM, HIDDEN_DIM, len(phonemes_dict)).cuda()\r\n",
        "model.load_state_dict(torch.load(path + 'model_test.pt'))\r\n",
        "model.eval()"
      ],
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LSTMModel(\n",
              "  (lstm): LSTM(20, 200)\n",
              "  (hidden2tag): Linear(in_features=200, out_features=43, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 158
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bXayNBWdOKQB",
        "outputId": "230d3b04-4a9e-49ee-ba70-e462d389a247"
      },
      "source": [
        "eval_model(model, zip(X_test_ph, y_test_ph))"
      ],
      "execution_count": 212,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1268it [00:00, 1794.83it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.845719740332514"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 212
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mi9HdAhYxYeC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CAv_eOP8xYSe"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wPTb4taDpGJx"
      },
      "source": [
        "pip install python-Levenshtein"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tUPhJx2upMLH"
      },
      "source": [
        "import Levenshtein\r\n",
        "from sklearn import metrics"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mR5KFRqvXdev"
      },
      "source": [
        "len_word = [0]\r\n",
        "for y in y_test_ph:\r\n",
        "    len_word.append(len_word[-1] + len(y))"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6O8mCKcjPAW"
      },
      "source": [
        "def show_metric_ph(y_pred, y_true, metric='micro'):\r\n",
        "\r\n",
        "    lev_score = []\r\n",
        "\r\n",
        "    for i in range(len(len_word) - 1):\r\n",
        "        pred = \"\".join([num_to_phonemes.get(s) for s in y_pred[len_word[i]:len_word[i+1]]])\r\n",
        "        true = \"\".join([num_to_phonemes.get(s) for s in y_true[len_word[i]:len_word[i+1]]])\r\n",
        "\r\n",
        "        lev_score.append(Levenshtein.distance(pred, true))    \r\n",
        "\r\n",
        "    print('\\nF1-score (%s):\\t%0.3f' % (metric, metrics.f1_score(y_true, y_pred, average=metric)))\r\n",
        "    print('Recall (%s): \\t%0.3f' % (metric, metrics.recall_score(y_true, y_pred, average=metric)))\r\n",
        "    print('Precision (%s):\\t%0.3f' % (metric,metrics.precision_score(y_true, y_pred, average=metric)))\r\n",
        "    print('Accuracy:\\t\\t%0.3f' % metrics.accuracy_score(y_true, y_pred))\r\n",
        "    print('\\nLevenshtein distance:\\t%0.3f' % np.mean(lev_score))"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pTsRuAOAnwiW"
      },
      "source": [
        "def fit_clf(clf, X_train, y_train):\r\n",
        "    clf.fit(X_train, y_train)\r\n",
        "    return clf\r\n",
        "\r\n",
        "def predict_clf(clf, X_test, y_test, metric='micro'):\r\n",
        "    y_pred = clf.predict(X_test)\r\n",
        "    show_metric_ph(y_pred, y_test, metric)\r\n",
        "    return y_pred"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CVVfbUg5pxsi",
        "outputId": "f83c28d8-bafc-403a-e18e-f613c746610d"
      },
      "source": [
        "y_pred = []\r\n",
        "y_true = []\r\n",
        "\r\n",
        "with torch.no_grad():\r\n",
        "    for features, labels in tqdm(zip(X_test_ph, y_test_ph)):\r\n",
        "        outputs = model(torch.tensor(features).cuda())            \r\n",
        "        outputs = outputs.detach().cpu().numpy().argmax(axis=1)\r\n",
        "\r\n",
        "        y_pred.extend(outputs)\r\n",
        "        y_true.extend(labels)\r\n",
        "\r\n",
        "show_metric_ph(y_pred, y_true, 'weighted') # 'weighted' - чуть ниже значения, но разные"
      ],
      "execution_count": 211,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1268it [00:00, 1690.80it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "F1-score (weighted):\t0.837\n",
            "Recall (weighted): \t0.841\n",
            "Precision (weighted):\t0.843\n",
            "Accuracy:\t\t0.841\n",
            "\n",
            "Levenshtein distance:\t0.943\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "biXOs18BxTfY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_s-c_X8DS820"
      },
      "source": [
        "## Классификаторы"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BANG5glggz5w"
      },
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.linear_model import LogisticRegression"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-gjp3j_fBqqF"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FnRyIfGEunRi"
      },
      "source": [
        "X_train_ph_2 = [x for sublist in X_train_ph for x in sublist]\r\n",
        "y_train_ph_2 = [x for sublist in y_train_ph for x in sublist]\r\n",
        "\r\n",
        "X_test_ph_2 = [x for sublist in X_test_ph for x in sublist]\r\n",
        "y_test_ph_2 = [x for sublist in y_test_ph for x in sublist]"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_j8c3Jf4unJ9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7254670c-4fef-45f2-f096-bac92a53775e"
      },
      "source": [
        "# Decision Tree\r\n",
        "\r\n",
        "tree_0 = DecisionTreeClassifier()\r\n",
        "\r\n",
        "tree_0 = fit_clf(tree_0, X_train_ph_2, y_train_ph_2)\r\n",
        "y_pred = predict_clf(tree_0, X_test_ph_2, y_test_ph_2) #'weighted'"
      ],
      "execution_count": 195,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "F1-score (micro):\t0.980\n",
            "Recall (micro): \t0.980\n",
            "Precision (micro):\t0.980\n",
            "Accuracy:\t\t0.980\n",
            "\n",
            "Levenshtein distance:\t0.029\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xc_9a6HeTzuH",
        "outputId": "bbeb4f19-f5db-4c0a-baf5-45d895acddb6"
      },
      "source": [
        "tree_0 = DecisionTreeClassifier()\r\n",
        "\r\n",
        "tree_0 = fit_clf(tree_0, X_train_ph_2, y_train_ph_2)\r\n",
        "y_pred = predict_clf(tree_0, X_test_ph_2, y_test_ph_2, 'weighted') #'weighted'"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "F1-score (weighted):\t0.977\n",
            "Recall (weighted): \t0.977\n",
            "Precision (weighted):\t0.977\n",
            "Accuracy:\t\t0.977\n",
            "\n",
            "Levenshtein distance:\t0.123\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t5ibjcPiunHK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "584701f7-72b3-4bc6-b5b0-42583f770594"
      },
      "source": [
        "# Random Forest\r\n",
        "\r\n",
        "rf_0 = RandomForestClassifier(n_estimators=15)\r\n",
        "\r\n",
        "rf_0 = fit_clf(rf_0, X_train_ph_2, y_train_ph_2)\r\n",
        "y_pred = predict_clf(rf_0, X_test_ph_2, y_test_ph_2, 'weighted') #'weighted'"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "F1-score (weighted):\t0.959\n",
            "Recall (weighted): \t0.960\n",
            "Precision (weighted):\t0.960\n",
            "Accuracy:\t\t0.960\n",
            "\n",
            "Levenshtein distance:\t0.222\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tOJMrgtwzNa3",
        "outputId": "e99bfd33-4b56-4efc-8692-84a60fdc9550"
      },
      "source": [
        "# Naive Baies\r\n",
        "\r\n",
        "nb_0 = GaussianNB(var_smoothing=1e-5) \r\n",
        "\r\n",
        "nb_0 = fit_clf(nb_0, X_train_ph_2, y_train_ph_2)\r\n",
        "y_pred = predict_clf(nb_0, X_test_ph_2, y_test_ph_2, 'weighted')"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "F1-score (weighted):\t0.469\n",
            "Recall (weighted): \t0.481\n",
            "Precision (weighted):\t0.509\n",
            "Accuracy:\t\t0.481\n",
            "\n",
            "Levenshtein distance:\t2.811\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BVOMbl5bT8zA",
        "outputId": "3fb71ace-fd16-4258-e0cc-70eb5a9deea4"
      },
      "source": [
        "# Logistic Regression\r\n",
        "\r\n",
        "lr_0 = LogisticRegression()\r\n",
        "\r\n",
        "lr_0 = fit_clf(lr_0, X_train_ph_2, y_train_ph_2)\r\n",
        "y_pred = predict_clf(lr_0, X_test_ph_2, y_test_ph_2, 'weighted')"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "F1-score (weighted):\t0.264\n",
            "Recall (weighted): \t0.307\n",
            "Precision (weighted):\t0.246\n",
            "Accuracy:\t\t0.307\n",
            "\n",
            "Levenshtein distance:\t4.014\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UGcOvwg6isBe"
      },
      "source": [
        "#Классификация фонем"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KLqcXG-7njFq"
      },
      "source": [
        "def fit_clf(clf, X_train, y_train):\n",
        "    clf.fit(X_train, y_train)\n",
        "    return clf\n",
        "\n",
        "def predict_clf(clf, X_test, y_test, metric='micro'):\n",
        "    y_pred = clf.predict(X_test)\n",
        "    show_metric(y_pred, y_test, metric)\n",
        "    return y_pred\n",
        "\n",
        "def show_metric(y_pred, y_true, metric):\n",
        "\n",
        "    lev_score = []\n",
        "    for i in range(len(len_word) - 1):\n",
        "        pred = \"\".join([num_to_allophones.get(s) for s in y_pred[len_word[i]:len_word[i+1]]])\n",
        "        true = \"\".join([num_to_allophones.get(s) for s in y_true[len_word[i]:len_word[i+1]]])\n",
        "        lev_score.append(Levenshtein.distance(pred, true))    \n",
        "\n",
        "    print('F1-score (%s):\\t%0.3f' % (metric, metrics.f1_score(y_true, y_pred, average=metric)))\n",
        "    print('Recall (%s): \\t%0.3f' % (metric, metrics.recall_score(y_true, y_pred, average=metric)))\n",
        "    print('Precision (%s):\\t%0.3f' % (metric,metrics.precision_score(y_true, y_pred, average=metric)))\n",
        "    print('Accuracy:\\t\\t%0.3f' % metrics.accuracy_score(y_true, y_pred))\n",
        "    print('\\nLevenshtein distance:\\t%0.3f' % np.mean(lev_score))"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LlHx1Vm1i6NL"
      },
      "source": [
        "## Decision Tree"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j-zop2EGHPTP",
        "outputId": "b4fe1c76-55d2-4812-f9ba-c3a6c4238e6a"
      },
      "source": [
        "# критерий разделения, максимальная глубина дерева, минимальное число объектов в листе, максимальное число листьев\n",
        "#tree = DecisionTreeClassifier(criterion='entropy', max_depth=5, min_samples_leaf=5, max_leaf_nodes=100) \n",
        "#tree = DecisionTreeClassifier(criterion='entropy', max_depth=7, min_samples_leaf=15, max_leaf_nodes=25)\n",
        "tree = DecisionTreeClassifier()\n",
        "\n",
        "tree = fit_clf(tree, X_train_allph, y_train_allph)\n",
        "y_pred = predict_clf(tree, X_test_allph, y_test_allph, 'weighted') #'weighted'"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "F1-score (weighted):\t0.992\n",
            "Recall (weighted): \t0.992\n",
            "Precision (weighted):\t0.991\n",
            "Accuracy:\t\t0.992\n",
            "\n",
            "Levenshtein distance:\t0.041\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "beaCtn5Wi8K-"
      },
      "source": [
        "## Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xisO3HDg5LPQ",
        "outputId": "24c19c30-6856-4fb2-88a8-f9fa92c21d2b"
      },
      "source": [
        "rf = RandomForestClassifier(n_estimators=15)\n",
        "\n",
        "rf = fit_clf(rf, X_train_allph, y_train_allph)\n",
        "y_pred = predict_clf(rf, X_test_allph, y_test_allph, 'weighted')"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "F1-score (weighted):\t0.973\n",
            "Recall (weighted): \t0.974\n",
            "Precision (weighted):\t0.973\n",
            "Accuracy:\t\t0.974\n",
            "\n",
            "Levenshtein distance:\t0.179\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TaLLlmsei-it"
      },
      "source": [
        "## Naive Baies \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wQFaZu8Bm8Oq",
        "outputId": "3bbe24d8-52e6-41ca-a6c2-c6210e0e007e"
      },
      "source": [
        "nb = GaussianNB(var_smoothing=1e-5) \n",
        "\n",
        "nb = fit_clf(nb, X_train_allph, y_train_allph)\n",
        "y_pred = predict_clf(nb, X_test_allph, y_test_allph, 'weighted')"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "F1-score (weighted):\t0.862\n",
            "Recall (weighted): \t0.858\n",
            "Precision (weighted):\t0.871\n",
            "Accuracy:\t\t0.858\n",
            "\n",
            "Levenshtein distance:\t0.756\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwT6TDJ2jDbq"
      },
      "source": [
        "## Logisric Regression\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KpPP6aqEPZSK",
        "outputId": "ba436c66-90af-4b41-eb67-def513474fb3"
      },
      "source": [
        "lr = LogisticRegression()\n",
        "\n",
        "lr = fit_clf(lr, X_train_allph, y_train_allph)\n",
        "y_pred = predict_clf(lr, X_test_allph, y_test_allph, 'weighted')"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "F1-score (weighted):\t0.303\n",
            "Recall (weighted): \t0.338\n",
            "Precision (weighted):\t0.300\n",
            "Accuracy:\t\t0.338\n",
            "\n",
            "Levenshtein distance:\t5.032\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}